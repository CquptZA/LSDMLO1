{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450380bf",
   "metadata": {},
   "source": [
    "# Pre and Compare sampling mthod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13493e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T07:35:22.026458Z",
     "start_time": "2023-11-07T07:35:21.830048Z"
    },
    "code_folding": [
     32,
     44,
     55,
     70,
     86,
     114,
     123,
     135,
     139,
     226,
     232,
     372,
     420,
     479,
     488,
     495
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.ensemble import RakelD\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.adapt import BRkNNbClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from enum import Enum\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.ensemble import RakelO\n",
    "from skmultilearn.adapt import MLTSVM\n",
    "from skmultilearn.adapt import MLARAM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from scipy import sparse\n",
    "import random\n",
    "from collections import Counter\n",
    "def CardAndDens(X,y):\n",
    "    cardmatrix=[]\n",
    "    for i in range(X.shape[0]):\n",
    "        count=0\n",
    "        for j in range(y.shape[1]):\n",
    "            if y[i,j]==1:\n",
    "                count+=1\n",
    "        cardmatrix.append(count)\n",
    "    Card=sum(cardmatrix)/len(cardmatrix)\n",
    "    Dens=Card/y.shape[1]\n",
    "    return Card,Dens\n",
    "def FeatureSelect(p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        featurecount=int(X.shape[1]*p)\n",
    "        Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "        Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "        featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "        new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "        new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def ImR(X,y):\n",
    "    Imr=[]\n",
    "    for i in range(y.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(y.shape[0]):\n",
    "            if y[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        if count1<=count0:\n",
    "            Imr.append(count0/count1)\n",
    "        else:\n",
    "            Imr.append(count1/count0)\n",
    "    return Imr\n",
    "def Imbalance(X,y):\n",
    "    countmatrix=[]\n",
    "    for i in range(y.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(y.shape[0]):\n",
    "            if y[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        countmatrix.append(count1)\n",
    "    maxcount=max(countmatrix)\n",
    "    ImbalanceRatioMatrix=[maxcount/i for i in countmatrix]\n",
    "    MaxIR=max(ImbalanceRatioMatrix)\n",
    "    MeanIR=sum(ImbalanceRatioMatrix)/len(ImbalanceRatioMatrix)\n",
    "    return ImbalanceRatioMatrix,MeanIR,countmatrix\n",
    "def Scumble(X,y):\n",
    "    ImbalanceRatioMatrix,MeanIR,_=Imbalance(X,y)\n",
    "    DifferenceImbalanceRatioMatrix=[i-MeanIR for i in ImbalanceRatioMatrix]\n",
    "    count=0\n",
    "    for i in range(y.shape[1]):\n",
    "        count+=math.pow(DifferenceImbalanceRatioMatrix[i],2)/(y.shape[1]-1)\n",
    "    ImbalanceRatioSigma=math.sqrt(count)\n",
    "    CVIR=ImbalanceRatioSigma/MeanIR\n",
    "    SumScumble=0\n",
    "    Scumble_i=[]\n",
    "    for i in range(y.shape[0]):\n",
    "        count=0\n",
    "        prod=1\n",
    "        SumIRLbl=0\n",
    "        for j in range(y.shape[1]):\n",
    "            IRLbl=1\n",
    "            if y[i,j]==1:\n",
    "                IRLbl=ImbalanceRatioMatrix[j]\n",
    "                SumIRLbl+=IRLbl\n",
    "                prod*=IRLbl\n",
    "                count+=1\n",
    "        if count==0:\n",
    "            Scumble_i.append(0)\n",
    "        else:\n",
    "            IRLbl_i=SumIRLbl/count\n",
    "            Scumble_i.append(1.0-((1.0/IRLbl_i) * math.pow(prod, 1.0/count)))\n",
    "    scumble=sum(Scumble_i)/X.shape[0]\n",
    "    return Scumble_i,scumble,CVIR\n",
    "def Labeltype(X,y):\n",
    "    ImbalanceRatioMatrix,MeanIR,_=Imbalance(X,y)\n",
    "    DifferenceImbalanceRatioMatrix=[i-MeanIR for i in ImbalanceRatioMatrix]\n",
    "    MinLabelIndex=[]\n",
    "    MajLabelIndex=[]\n",
    "    count=0\n",
    "    for i in (DifferenceImbalanceRatioMatrix):\n",
    "        if i>0:\n",
    "            MinLabelIndex.append(count)\n",
    "        else:\n",
    "            MajLabelIndex.append(count)\n",
    "        count+=1\n",
    "    MinLabelName=[]\n",
    "    MajLabelName=[]\n",
    "    for i in MinLabelIndex:\n",
    "        MinLabelName.append(label_names[i][0])\n",
    "    for i in MajLabelIndex:\n",
    "        MajLabelName.append(label_names[i][0])\n",
    "    MinLabeldic=dict(zip(MinLabelIndex,MinLabelName))\n",
    "    MajLabeldic=dict(zip(MajLabelIndex,MajLabelName))\n",
    "    return MinLabeldic,MajLabeldic\n",
    "def CalcuNN(df1,n_neighbor):\n",
    "    nbs=NearestNeighbors(n_neighbors=n_neighbor,metric='euclidean',algorithm='kd_tree').fit(df1)\n",
    "    euclidean,indices= nbs.kneighbors(df1)\n",
    "    return euclidean,indices\n",
    "def MLSMOTE(df1,df2,n_neighbor,method):\n",
    "    start_time = time.time()\n",
    "    ML_SMOTE_new_X=df1.copy(deep=True)\n",
    "    ML_SMOTE_target=df2.copy(deep=True)\n",
    "    MinLabeldic,MajLabeldic=Labeltype(np.array(df1),np.array(df2))\n",
    "    ImbalanceRatioMatrix,MeanIR,countmatrix=Imbalance(np.array(df1),np.array(df2))\n",
    "    MinLabelindex=list(MinLabeldic.keys())\n",
    "    tem=['True']*len(MinLabelindex)\n",
    "    dic=dict(zip(MinLabelindex,tem))\n",
    "    minlabels=[]\n",
    "    for i in range(df2.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(df2.shape[0]):\n",
    "            if df2.iloc[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        if count1<=count0:\n",
    "            minlabels.append(1)\n",
    "        else:\n",
    "            minlabels.append(0)\n",
    "    np.random.seed(100)\n",
    "    for tail_label in MinLabelindex:\n",
    "        if(dic[tail_label]=='False'):\n",
    "            continue        \n",
    "        new_IRLbl=max(countmatrix)/countmatrix[tail_label]\n",
    "        if(new_IRLbl<=MeanIR):\n",
    "            dic[tail_label]='False'\n",
    "            continue  \n",
    "        sub_index=list(df2[df2[MinLabeldic[tail_label]]==minlabels[tail_label]].index)\n",
    "        dfX= df1[df1.index.isin(sub_index)].reset_index(drop = True)\n",
    "        dfy= df2[df2.index.isin(sub_index)].reset_index(drop = True)\n",
    "        new_X = np.zeros((dfX.shape[0], dfX.shape[1]))\n",
    "        target = np.zeros((dfy.shape[0], dfy.shape[1]))\n",
    "        count=0\n",
    "        if dfX.shape[0]==1:\n",
    "            new_X[count]=np.array(dfX)\n",
    "            target[count]=np.array(dfy)\n",
    "            count+=1\n",
    "            break\n",
    "        if(dfX.shape[0]>n_neighbor):\n",
    "            euclidean,indices=CalcuNN(dfX,n_neighbor+1)\n",
    "        else:\n",
    "            euclidean,indices=CalcuNN(dfX,dfX.shape[0])\n",
    "        for i in range(dfX.shape[0]):\n",
    "            seed = i\n",
    "            reference = np.random.choice(indices[seed,1:])\n",
    "            all_point = indices[seed,:]\n",
    "            nn_df = dfy[dfy.index.isin(all_point)]\n",
    "            ser = nn_df.sum(axis = 0, skipna = True)      \n",
    "            for j in range(dfX.shape[1]): \n",
    "                reference = np.random.choice(indices[seed,1:])\n",
    "                ratio=np.random.random()\n",
    "                if feature_names[j][1]=='NUMERIC':  \n",
    "                    new_X[count,j] = dfX.iloc[seed,j] + ratio * (dfX.iloc[reference,j] - dfX.iloc[seed,j])\n",
    "                elif feature_names[j][1]==['YES', 'NO'] or feature_names[j][1]==['0', '1']:\n",
    "                    if ratio>=0.5:\n",
    "                        new_X[count,j] =dfX.iloc[seed,j]\n",
    "                    else:\n",
    "                        new_X[count,j] =dfX.iloc[reference,j]\n",
    "                else:\n",
    "                    new_X[count,j] =dfX.iloc[seed,j]\n",
    "            if (method==\"Ranking\"):\n",
    "                target[count] = np.array([1 if val>=((n_neighbor+1)/2) else 0 for val in ser])\n",
    "            elif (method==\"Union\"):\n",
    "                target[count] = np.array([1 if val>0 else 0 for val in ser])\n",
    "            else:   \n",
    "                target[count] = np.array([1 if val==len(all_point) else 0 for val in ser]) \n",
    "            for k in MinLabelindex:\n",
    "                if target[count,k]==minlabels[k]:\n",
    "                    countmatrix[k]+=1\n",
    "            count+=1\n",
    "        new_X = pd.DataFrame(new_X,columns=[x[0] for x in feature_names])\n",
    "        target = pd.DataFrame(target,columns=[y[0] for y in label_names])\n",
    "        dropindex=[]\n",
    "        for i in range(target.shape[0]):\n",
    "            d=np.array(df2.iloc[i:])\n",
    "            if d.sum()==0:\n",
    "                dropindex.append(i)\n",
    "        new_X.drop(dropindex).reset_index(drop=True)\n",
    "        target.drop(dropindex).reset_index(drop=True)\n",
    "        ML_SMOTE_new_X = pd.concat([ML_SMOTE_new_X, new_X], axis=0).reset_index(drop=True)\n",
    "        ML_SMOTE_target = pd.concat([ML_SMOTE_target, target], axis=0).reset_index(drop=True)\n",
    "    end_time = time.time()\n",
    "    times=end_time-start_time\n",
    "    return ML_SMOTE_new_X,ML_SMOTE_target\n",
    "class LocalType(Enum):\n",
    "    Safe=1\n",
    "    BoderLine=2\n",
    "    Rare=3\n",
    "    Outlier=4\n",
    "    Majority=0\n",
    "def MLSOL(df1,df2,p):\n",
    "    start_time = time.time()\n",
    "    np.random.seed(21)\n",
    "    n_neighbors=5\n",
    "    nbs=NearestNeighbors(n_neighbors=n_neighbors+1,metric='euclidean',algorithm='kd_tree').fit(df1)\n",
    "    euclidean2,indices2= nbs.kneighbors(df1)\n",
    "    minlabels=[]\n",
    "    for i in range(df2.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(df2.shape[0]):\n",
    "            if df2.iloc[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        if count1<=count0:\n",
    "            minlabels.append(1)\n",
    "        else:\n",
    "            minlabels.append(0)      \n",
    "    C=np.zeros((df2.shape[0],df2.shape[1]))\n",
    "    for i in range(df2.shape[0]): \n",
    "        all_neighbour = indices2[i,1:]\n",
    "        for j in range(df2.shape[1]):\n",
    "            if df2.iloc[i,j]==minlabels[j]:\n",
    "                count=0\n",
    "                for k in all_neighbour.tolist():\n",
    "                    if df2.iloc[i,j]!=df2.iloc[k,j]:\n",
    "                        count+=1\n",
    "                C[i,j]=(count)/n_neighbors\n",
    "            else:\n",
    "                C[i,j]=0\n",
    "    W=np.zeros(df1.shape[0])\n",
    "    tem=np.zeros([df2.shape[0],df2.shape[1]])\n",
    "    for j in range(df2.shape[1]):    \n",
    "        SumC=0.0\n",
    "        c=0\n",
    "        for i in range(df2.shape[0]):\n",
    "            if C[i,j]<1 and C[i,j]!=0:\n",
    "                SumC+=C[i,j]\n",
    "                c+=1\n",
    "        if SumC!=0.0 and c!=0:\n",
    "            for i in range(df2.shape[0]):\n",
    "                if C[i,j]<1 and C[i,j]!=0:\n",
    "                    tem[i,j]=C[i,j]/SumC\n",
    "        else:\n",
    "            tem[i,j]=0\n",
    "    SumW=0\n",
    "    for i in range(df2.shape[0]):\n",
    "        for j in range(df2.shape[1]):\n",
    "            if tem[i,j]!=0:\n",
    "                W[i]+=tem[i,j]\n",
    "        SumW+=W[i]\n",
    "    InstanceType=np.zeros([df2.shape[0],df2.shape[1]])\n",
    "    for i in range(df2.shape[0]):\n",
    "        for j in range(df2.shape[1]):\n",
    "            if df2.iloc[i,j]==minlabels[j]:\n",
    "                if C[i,j]<0.3:\n",
    "                    InstanceType[i,j]=LocalType.Safe.value\n",
    "                elif C[i,j]<0.7:\n",
    "                    InstanceType[i,j]=LocalType.BoderLine.value\n",
    "                elif C[i,j]<1:\n",
    "                    InstanceType[i,j]=LocalType.Rare.value\n",
    "                elif C[i,j]==1:\n",
    "                    InstanceType[i,j]=LocalType.Outlier.value\n",
    "# majority class\n",
    "            else:\n",
    "                InstanceType[i,j]=LocalType.Majority.value \n",
    "# reanalyse\n",
    "#     for i in range(df2.shape[0]):\n",
    "#         for j in range(df2.shape[1]):\n",
    "#             if InstanceType[i,j]==LocalType.Rare.value:\n",
    "#                 for k in indices2[i,1:]:\n",
    "#                     if (InstanceType[k,j]==LocalType.Safe.value or InstanceType[k,j]==LocalType.Safe.value):\n",
    "#                         InstanceType[i,j]=LocalType.BoderLine.value\n",
    "#                         break                   \n",
    "    n_sample=int(df1.shape[0]*p)\n",
    "    new_X = np.zeros((n_sample, df1.shape[1]))\n",
    "    target = np.zeros((n_sample, df2.shape[1]))\n",
    "    for i in range(n_sample):\n",
    "        random_count=np.random.random()*SumW\n",
    "        seed=0\n",
    "        s=0\n",
    "        for k in range(len(W)):\n",
    "            s+=W[k]\n",
    "            if(random_count<=s):\n",
    "                seed=k\n",
    "                break            \n",
    "   #     生成样本空间\n",
    "        for j in range(df1.shape[1]): \n",
    "    #         随机选取一个近邻\n",
    "            reference = np.random.choice(indices2[seed,1:])\n",
    "            ratio=np.random.random()\n",
    "            if feature_names[j][1]=='NUMERIC':  \n",
    "                new_X[i,j] = df1.iloc[seed,j] + ratio*(df1.iloc[reference,j]-df1.iloc[seed,j])\n",
    "            elif feature_names[j][1]==['YES', 'NO'] or feature_names[j][1]==['0', '1']:\n",
    "                rmd=np.random.choice([True, False])\n",
    "                if rmd==True:\n",
    "                    new_X[i,j]=df1.iloc[seed,j] \n",
    "                else:\n",
    "                    new_X[i,j]=df1.iloc[reference,j]\n",
    "            else:\n",
    "                new_X[i,j]=df1.iloc[seed,j]\n",
    "    #     生成样本和种子实例的欧氏距离\n",
    "        dist1 = np.linalg.norm(new_X[i]-np.array(df1.loc[seed,:]))\n",
    "    #     生成样本和近邻实例的欧氏距离\n",
    "        dist2 = np.linalg.norm(new_X[i]-np.array(df1.loc[reference,:]))\n",
    "    #     计算cd\n",
    "        cd=dist1/(dist1+dist2)\n",
    "        npseed=np.array(df2.loc[seed])\n",
    "        npreference=np.array(df2.loc[reference])\n",
    "#     生成标签空间\n",
    "        for j in range(df2.shape[1]):\n",
    "            if npseed[j]==npreference[j]:\n",
    "                target[i,j]=df2.iloc[seed,j]\n",
    "            else:\n",
    "                theta=0.5\n",
    "                if InstanceType[seed,j]==LocalType.Majority.value:\n",
    "                    npseed[j],npreference[j]=npreference[j],npseed[j]\n",
    "                    cd=1-cd\n",
    "                else:\n",
    "                    if InstanceType[seed,j]==LocalType.Safe.value:\n",
    "                        theta=0.5\n",
    "                    elif InstanceType[seed,j]==LocalType.BoderLine.value:\n",
    "                        theta=0.75\n",
    "                    elif InstanceType[seed,j]==LocalType.Rare.value:\n",
    "                        theta=1+(1e-5)\n",
    "                    else:\n",
    "                        theta=0-(1e-5)\n",
    "                if cd<=theta:\n",
    "                    target[i,j]=npseed[j]\n",
    "                else:\n",
    "                    target[i,j]=npreference[j]\n",
    "    new_X = pd.DataFrame(new_X,columns=[x[0] for x in feature_names])\n",
    "    target = pd.DataFrame(target,columns=[y[0] for y in label_names])\n",
    "    ML_SOL_new_X = pd.concat([df1, new_X], axis=0).reset_index(drop=True)\n",
    "    ML_SOL_target = pd.concat([df2, target], axis=0).reset_index(drop=True)\n",
    "    end_time = time.time()\n",
    "    times=end_time-start_time\n",
    "    return ML_SOL_new_X,ML_SOL_target\n",
    "def MLROS(df1,df2,p):\n",
    "    start_time = time.time()\n",
    "    np.random.seed(10)\n",
    "    SamplesToClone=int(df1.shape[0]*p)\n",
    "    sub_index=list()\n",
    "    ImbalanceRatioMatrix,MeanIR,countmatrix=Imbalance(np.array(df1),np.array(df2))\n",
    "    MinLabeldic,_=Labeltype(np.array(df1),np.array(df2))\n",
    "    ML_ROS_new_X=df1.copy(deep=True)\n",
    "    ML_ROS_target=df2.copy(deep=True)\n",
    "    MinLabelindex=list(MinLabeldic.keys())\n",
    "    tem=['True']*len(MinLabelindex)\n",
    "    dic=dict(zip(MinLabelindex,tem))\n",
    "    minlabels=[]\n",
    "    for i in range(df2.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(df2.shape[0]):\n",
    "            if df2.iloc[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        if count1<=count0:\n",
    "            minlabels.append(1)\n",
    "        else:\n",
    "            minlabels.append(0)      \n",
    "    while(SamplesToClone>0):\n",
    "        for tail_label_index in MinLabelindex:\n",
    "            if(dic[tail_label_index]=='False'):\n",
    "                continue        \n",
    "            new_IRLbl=max(countmatrix)/countmatrix[tail_label_index]\n",
    "            if(new_IRLbl<=MeanIR):\n",
    "                dic[tail_label_index]='False'\n",
    "                continue  \n",
    "            sub_index=df2[df2[MinLabeldic[tail_label_index]]==minlabels[tail_label_index]].index\n",
    "            randomindex=np.random.choice(sub_index)\n",
    "            for index in MinLabelindex:\n",
    "                if(df2.iloc[randomindex,index]==minlabels[tail_label_index]):\n",
    "                    countmatrix[index]+=1\n",
    "            ML_ROS_new_X = pd.concat([ML_ROS_new_X, df1.loc[[randomindex]]], axis=0).reset_index(drop=True)\n",
    "            ML_ROS_target = pd.concat([ML_ROS_target, df2.loc[[randomindex]]], axis=0).reset_index(drop=True)\n",
    "            SamplesToClone=SamplesToClone-1\n",
    "            if(SamplesToClone<=0):\n",
    "                break  \n",
    "        if(set(dic.values())==({'False'})):\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    times=end_time-start_time\n",
    "    return ML_ROS_new_X,ML_ROS_target\n",
    "def MLRUS(df1,df2,p):\n",
    "    start_time = time.time()\n",
    "    np.random.seed(10)\n",
    "    SamplesToDelete=int(df1.shape[0]*p)\n",
    "    sub_index=list()\n",
    "    DeleteIndex=list()\n",
    "    ImbalanceRatioMatrix,MeanIR,countmatrix=Imbalance(np.array(df1),np.array(df2))\n",
    "    MinLabeldic,MajLabeldic=Labeltype(np.array(df1),np.array(df2))\n",
    "    MinLabelindex=list(MinLabeldic.keys())\n",
    "    minlabels=[]\n",
    "    for i in range(df2.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(df2.shape[0]):\n",
    "            if df2.iloc[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        if count1<=count0:\n",
    "            minlabels.append(1)\n",
    "        else:\n",
    "            minlabels.append(0)     \n",
    "    protectins=[]\n",
    "    for min_index in MinLabelindex:\n",
    "        if df2.iloc[:,min_index].sum()==1:\n",
    "            for j in range(df2.shape[0]):\n",
    "                if df2.iloc[j,min_index]==1:\n",
    "                    protectins.append(j) \n",
    "    ML_RUS_new_X=df1.copy(deep=True)\n",
    "    ML_RUS_target=df2.copy(deep=True)\n",
    "    MajLabelindex=list(MajLabeldic.keys())\n",
    "    tem=['True']*len(MajLabelindex)\n",
    "    dic=dict(zip(MajLabelindex,tem))\n",
    "    while(SamplesToDelete>0):\n",
    "        for maj_index in MajLabelindex:\n",
    "            if(dic[maj_index]=='False'):\n",
    "                continue        \n",
    "            new_IRLbl=max(countmatrix)/countmatrix[maj_index]\n",
    "            if(new_IRLbl>MeanIR):\n",
    "                dic[maj_index]='False'\n",
    "                continue  \n",
    "            sub_index=df2[df2[MajLabeldic[maj_index]]==minlabels[maj_index]].index\n",
    "            randomindex=np.random.choice(sub_index)\n",
    "            if(randomindex in DeleteIndex) or (randomindex in protectins):\n",
    "                continue\n",
    "            DeleteIndex.append(randomindex)\n",
    "            for index in MajLabelindex:\n",
    "                if(df2.iloc[randomindex,index]==minlabels[maj_index]):\n",
    "                    countmatrix[index]-=1\n",
    "            SamplesToDelete=SamplesToDelete-1\n",
    "            if(SamplesToDelete<=0):\n",
    "                break  \n",
    "        if(set(dic.values())==({'False'})):\n",
    "            break\n",
    "    ML_RUS_new_X = ML_RUS_new_X.drop(index=DeleteIndex).reset_index(drop=True)\n",
    "    ML_RUS_target = ML_RUS_target.drop(index=DeleteIndex).reset_index(drop=True)\n",
    "    end_time = time.time()\n",
    "    times=end_time-start_time\n",
    "    return ML_RUS_new_X,ML_RUS_target\n",
    "def REMEDIAL(df1,df2):\n",
    "    Scumble_i,scumble,_=Scumble(np.array(df1),np.array(df2))\n",
    "    MinLabeldic,MajLabeldic=Labeltype(np.array(df1),np.array(df2))\n",
    "    allindex=[i for i,scumble_i in enumerate(Scumble_i) if scumble_i>scumble]\n",
    "    REMEDIAL_X=df1.copy(deep=True)\n",
    "    REMEDIAL_target=df2.copy(deep=True)\n",
    "    change_x=df1.iloc[allindex]\n",
    "    change_y=df2.iloc[allindex]\n",
    "    for i in allindex:\n",
    "        for tail_label in MinLabeldic.values():\n",
    "            REMEDIAL_target.loc[i,tail_label]=0        \n",
    "        for maj_label in MajLabeldic.values():\n",
    "            change_y.loc[i,maj_label]=0\n",
    "    new_X=pd.concat([REMEDIAL_X,change_x],axis=0).reset_index(drop=True)\n",
    "    new_target=pd.concat([REMEDIAL_target,change_y],axis=0).reset_index(drop=True)\n",
    "    return new_X,new_target\n",
    "def diflabel(df1,df2):\n",
    "    nbs=NearestNeighbors(n_neighbors=df1.shape[0],metric='euclidean',algorithm='kd_tree').fit(df1)\n",
    "    euclidean,indices= nbs.kneighbors(df1)\n",
    "    cnt=[]\n",
    "    for i in range(df1.shape[0]):\n",
    "        count=0\n",
    "        for j in indices[i,1:6]:\n",
    "            if np.array_equal(df2.iloc[i], df2.iloc[j]):\n",
    "                count+=1\n",
    "        cnt.append(count/5)\n",
    "    print(sum(cnt)/len(cnt))\n",
    "    print(sum((x -(sum(cnt)/len(cnt))) ** 2 for x in cnt) / len(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447e720",
   "metadata": {},
   "source": [
    "# Compare base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23a4bb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T07:35:23.496633Z",
     "start_time": "2023-11-07T07:35:23.472911Z"
    },
    "code_folding": [
     10,
     25,
     36,
     38,
     43,
     48,
     123,
     125,
     130,
     148,
     214
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "import pdb\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "class Fd(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple fully connected network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, H, out_dim, fin_act=None):\n",
    "        super(Fd, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, H)\n",
    "        self.fc2 = torch.nn.Linear(H, out_dim)\n",
    "        self.fin_act = fin_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.fin_act(x) if self.fin_act else x\n",
    "class Fx(torch.nn.Module):\n",
    "    def __init__(self, in_dim, H1, H2, out_dim):\n",
    "        super(Fx, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, H1)\n",
    "        self.fc2 = torch.nn.Linear(H1, H2)\n",
    "        self.fc3 = torch.nn.Linear(H2, out_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return x\n",
    "class Fe(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, H, out_dim):\n",
    "        super(Fe, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, H)\n",
    "        self.fc2 = torch.nn.Linear(H, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return x\n",
    "# C2AE\n",
    "class C2AE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, Fx, Fe, Fd, beta=1, alpha=.5, emb_lambda=.5, latent_dim=6,\n",
    "                 device=None):\n",
    "        super(C2AE, self).__init__()\n",
    "        # Define main network components.\n",
    "        # Encodes x into latent space. X ~ z\n",
    "        self.Fx = Fx\n",
    "        # Encodes y into latent space. Y ~ z\n",
    "        self.Fe = Fe\n",
    "        # Decodes latent space into Y. z ~ Y\n",
    "        self.Fd = Fd\n",
    "\n",
    "        # Hyperparam used to set tradeoff between latent loss, and corr loss.\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # Lagrange to use in embedding loss.\n",
    "        self.emb_lambda = emb_lambda\n",
    "        self.latent_I = torch.eye(latent_dim).to(device)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        if self.training:\n",
    "            # Calculate feature, and label latent representations.\n",
    "            fx_x = self.Fx(x)\n",
    "            fe_y = self.Fe(y)\n",
    "            # Calculate decoded latent representation.\n",
    "            fd_z = self.Fd(fe_y)\n",
    "            return fx_x, fe_y, fd_z\n",
    "        else:\n",
    "            # If evaluating just send through encoder and decoder.\n",
    "            return self.predict(x)\n",
    "\n",
    "    def _predict(self, y):\n",
    "        return self.Fd(self.Fe(y))\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.Fd(self.Fx(x))\n",
    "\n",
    "    def corr_loss(self, preds, y):\n",
    "        # Generate masks for [0,1] elements.\n",
    "        ones = (y == 1)\n",
    "        zeros = (y == 0)\n",
    "        # Use broadcasting to apply logical and between mask arrays.\n",
    "        # This will only indicate locations where both masks are 1.\n",
    "        # THis corresponds to set we are enumerating in eq (3) in Yah et al.\n",
    "        ix_matrix = ones[:, :, None] & zeros[:, None, :]\n",
    "        # Use same broadcasting logic to generate exponetial differences.\n",
    "        # This like the above broadcast will do so between all pairs of points\n",
    "        # for every datapoint.\n",
    "        diff_matrix = torch.exp(-(preds[:, :, None] - preds[:, None, :]))\n",
    "        # This will sum all contributes to loss for each datapoint.\n",
    "        losses = torch.flatten(diff_matrix*ix_matrix, start_dim=1).sum(dim=1)\n",
    "        # Normalize each loss add small epsilon incase 0.\n",
    "        losses /= (ones.sum(dim=1)*zeros.sum(dim=1) + 1e-4)\n",
    "        # Replace inf, and nans with 0.\n",
    "        losses[losses == float('Inf')] = 0\n",
    "        losses[torch.isnan(losses)] = 0\n",
    "        # Combine all losses to retrieve final loss.\n",
    "        return losses.sum()\n",
    "\n",
    "    def latent_loss(self, fx_x, fe_y):\n",
    "        c1 = fx_x - fe_y\n",
    "        # Here to help hold constraint of FxFx^2 = FyFy^2 = I.\n",
    "        c2 = fx_x.T@fx_x - self.latent_I\n",
    "        c3 = fe_y.T@fe_y - self.latent_I\n",
    "        # Combine loss components as specified in Yah et al.\n",
    "        latent_loss = torch.trace(\n",
    "            c1@c1.T) + self.emb_lambda*torch.trace(c2@c2.T + c3@c3.T)\n",
    "        # ********** Version 2: Ignore constraint **********\n",
    "        #latent_loss = torch.mean((fx_x - fe_y)**2)\n",
    "        return latent_loss\n",
    "    def losses(self, fx_x, fe_y, fd_z, y):\n",
    "        l_loss = self.latent_loss(fx_x, fe_y)\n",
    "        c_loss = self.corr_loss(fd_z, y)\n",
    "        return l_loss, c_loss   \n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "def load_model(model_cls, path, *args, **kwargs):\n",
    "    model = model_cls(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "def eval_metrics(mod, metrics, datasets, device, apply_sig=False):\n",
    "    res_dict = {}\n",
    "    for ix, dataset in enumerate(datasets):\n",
    "        mod.eval()\n",
    "        x = dataset.tensors[0].to(device).float()\n",
    "        # Make predictions.\n",
    "        preds = mod(x)\n",
    "        y_prob = preds.cpu().detach().numpy()\n",
    "        # Convert them to binary multilabels.\n",
    "        if apply_sig:\n",
    "            y_pred = torch.round(torch.sigmoid(preds)).cpu().detach().numpy()\n",
    "        else:\n",
    "            y_pred = torch.round(preds).cpu().detach().numpy()\n",
    "        y_true = dataset.tensors[1].cpu().detach().numpy()\n",
    "        # Calculate metric.\n",
    "        res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_pred) for metric in metrics}\n",
    "    return res_dict\n",
    "# COCOA\n",
    "class COCOA:\n",
    "    np.random.seed(10)\n",
    "    def __init__(self):\n",
    "        self.classifiers = []\n",
    "        self.thresholds = []\n",
    "        self.mclassifiers=[]\n",
    "        self.mthresholds=[]\n",
    "        self.couples=3\n",
    "        self.classname=np.array([0,1,2])      \n",
    "    def find_optimal_threshold(self, X, Y):\n",
    "        self.classifiers = []\n",
    "        self.thresholds = []\n",
    "        num_labels = Y.shape[1]\n",
    "        for i in range(num_labels):\n",
    "            y = Y[:, i]\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X, y)\n",
    "            self.classifiers.append(clf)       \n",
    "            y_pred = clf.predict_proba(X)[:, 1]\n",
    "            f1_scores = []\n",
    "            thresholds = np.arange(0, 1.01, 0.01)\n",
    "            for threshold in thresholds:\n",
    "                y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "                f1 = f1_score(y, y_pred_binary, average='binary')\n",
    "                f1_scores.append(f1)\n",
    "            best_threshold_idx = np.argmax(f1_scores)\n",
    "            self.thresholds.append(thresholds[best_threshold_idx])\n",
    "    def fit(self, X, Y):\n",
    "        self.find_optimal_threshold(X, Y)\n",
    "        self.mclassifiers = []\n",
    "        couples=self.couples\n",
    "        num_labels = Y.shape[1]\n",
    "        for i in range(num_labels):\n",
    "            new_y=self.multilabel_to_multiclass(Y,i)\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X, new_y)\n",
    "            self.mclassifiers.append(clf)\n",
    "    def multilabel_to_multiclass(self, y, index):\n",
    "        np.random.seed(10)\n",
    "        couples=self.couples\n",
    "        classname=self.classname\n",
    "        new_y = np.zeros((y.shape[0],))\n",
    "        other_labels = np.delete(np.arange(y.shape[1]), index)\n",
    "        couple_labels = np.random.choice(other_labels, couples, replace=False)\n",
    "        index_label = y[:, index]     \n",
    "        new_y[index_label == 1] = classname[2]\n",
    "        couple_labels_present = (y[:, couple_labels] == 1).any(axis=1)\n",
    "        new_y[(index_label == 0) & couple_labels_present] = classname[1]\n",
    "        new_y[(index_label == 0) & (~couple_labels_present)] = classname[0]\n",
    "        return new_y.astype(int)\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        classname=self.classname\n",
    "        Y_pred = np.zeros((num_samples, len(self.mclassifiers)))        \n",
    "        for i, clf in enumerate(self.mclassifiers):\n",
    "            y_pred = clf.predict_proba(X)[:, -1]\n",
    "            Y_pred[:, i] = (y_pred >= self.thresholds[i]).astype(int)\n",
    "        return Y_pred \n",
    "    def predict_proba(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        classname=self.classname\n",
    "        Y_pred = np.zeros((num_samples, len(self.mclassifiers)))\n",
    "        for i, clf in enumerate(self.mclassifiers):\n",
    "            y_pred = clf.predict_proba(X)[:, -1]\n",
    "            Y_pred[:, i] = y_pred\n",
    "        return Y_pred\n",
    "class ECC:\n",
    "    def __init__(self):\n",
    "        self.num_classifiers = 10\n",
    "        self.labels = y.shape[1]\n",
    "        self.k =y.shape[1]\n",
    "        self.classifiers = []\n",
    "        self.classifiers_label_index = []\n",
    "        for _ in range(self.num_classifiers):\n",
    "            classifier = ClassifierChain(\n",
    "                classifier=XGBClassifier(random_state=42, n_jobs=-1, eta=0.05, max_depth=7, colsample_bytree=1,\n",
    "                                         n_estimators=377,\n",
    "                                         tree_method='gpu_hist'),\n",
    "                require_dense=[False, True]\n",
    "            )\n",
    "            self.classifiers.append(classifier)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        num_instances = X.shape[0]\n",
    "        num_labels = Y.shape[1]\n",
    "        original_list =list(range(self.labels))\n",
    "        for classifier in self.classifiers:\n",
    "            random_sorted_list = np.random.permutation(original_list)\n",
    "            self.classifiers_label_index.append(random_sorted_list)\n",
    "            selected_instances = np.random.choice(num_instances, size=int(1 * num_instances), replace=False)\n",
    "            classifier.fit(X[selected_instances], Y[selected_instances][:, random_sorted_list])\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        Y_pred = (probabilities >= threshold).astype(int)\n",
    "        return Y_pred\n",
    "    def predict_proba(self, X):\n",
    "        Y_pred = np.zeros((X.shape[0], self.labels))\n",
    "        i = 0\n",
    "        for classifier in self.classifiers:\n",
    "            proba_matrix = classifier.predict_proba(X)\n",
    "            for j, label in enumerate(self.classifiers_label_index[i]):\n",
    "                Y_pred[:, label] += proba_matrix[:, j].toarray().squeeze()\n",
    "            i = i + 1\n",
    "        Y_pred /= self.num_classifiers\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ddd65",
   "metadata": {},
   "source": [
    "# GET W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6551ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T07:35:27.762086Z",
     "start_time": "2023-11-07T07:35:27.724867Z"
    },
    "code_folding": [
     105,
     108
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "# Caculate W\n",
    "def CLML(X, Y, optmParameter):\n",
    "    # optimization parameters\n",
    "    alpha = optmParameter['alpha']\n",
    "    beta = optmParameter['beta']\n",
    "    gamma = optmParameter['gamma']\n",
    "    lamda = optmParameter['lamda']\n",
    "    lamda2 = optmParameter['lamda2']\n",
    "    maxIter = optmParameter['maxIter']\n",
    "    miniLossMargin = optmParameter['minimumLossMargin']\n",
    "    \n",
    "    # initialization\n",
    "    num_dim = X.shape[1]\n",
    "    XT = X.T\n",
    "    XTX = np.dot(XT, X)\n",
    "    XTY = np.dot(XT, Y)\n",
    "    W_s = np.linalg.solve(XTX + gamma*np.eye(num_dim), XTY)\n",
    "    W_s_1 = W_s\n",
    "    \n",
    "    # label correlation\n",
    "    R = pdist(Y.T+np.finfo(float).eps, metric='cosine')\n",
    "    R = 1 - squareform(R)\n",
    "    C = R.reshape(Y.shape[1], Y.shape[1])\n",
    "    L1 = np.diag(np.sum(C, axis=1)) - C\n",
    "    \n",
    "    S = ins_similarity(X, 5)\n",
    "    L2 = np.diag(np.sum(S, axis=1)) - S\n",
    "    iter = 1\n",
    "    oldloss = 0\n",
    "    bk = 1\n",
    "    bk_1 = 1\n",
    "    \n",
    "    # compute LIP\n",
    "    A = gradL21(W_s)\n",
    "#     Lip = np.sqrt(4*(np.linalg.norm(XTX)**2 + np.linalg.norm(alpha*XTX)**2 * np.linalg.norm(L1)**2 \n",
    "#                   + np.linalg.norm(lamda*XT.dot(L2).dot(X))**2) + np.linalg.norm(lamda2*A)**2)\n",
    "    Lip = np.sqrt(4*(np.linalg.norm(XTX)**2 + np.linalg.norm(alpha*XTX)**2 * np.linalg.norm(L1)**2))   \n",
    "    # proximal gradient\n",
    "    while iter <= maxIter:\n",
    "        A = gradL21(W_s)\n",
    "        W_s_k = W_s + (bk_1 - 1)/bk * (W_s - W_s_1)\n",
    "#         gradF = np.dot(XTX, W_s_k) - XTY + alpha * np.dot(XTX, W_s_k).dot(L1) + lamda * XT.dot(L2).dot(X).dot(W_s_k) + lamda2*A.dot(W_s_k)\n",
    "        gradF = np.dot(XTX, W_s_k) - XTY + alpha * np.dot(XTX, W_s_k).dot(L1) \n",
    "        Gw_s_k = W_s_k - 1/Lip *(gradF)\n",
    "        # update b, W\n",
    "        bk_1 = bk\n",
    "        bk = (1 + np.sqrt(4*bk**2 + 1))/2\n",
    "        W_s_1 = W_s\n",
    "        W_s = softthres(Gw_s_k, beta/Lip)\n",
    "        \n",
    "        # compute loss function\n",
    "        predictionLoss = np.trace(np.dot(X.dot(W_s) - Y, (X.dot(W_s) - Y).T))\n",
    "        F = X.dot(W_s)\n",
    "        correlation = np.trace(F.dot(L1).dot(F.T))\n",
    "        In_correlation = np.trace(F.T.dot(L2).dot(F))\n",
    "        sparsity = np.sum(W_s != 0)\n",
    "        sparsity2 = np.trace(np.dot(W_s.T, A).dot(W_s))\n",
    "        totalloss = predictionLoss + alpha*correlation + beta*sparsity \n",
    "#         totalloss = predictionLoss + alpha*correlation + beta*sparsity + lamda*In_correlation + lamda2*sparsity2\n",
    "        if abs(oldloss - totalloss) <= miniLossMargin or totalloss <= 0:\n",
    "            break\n",
    "        else:\n",
    "            oldloss = totalloss\n",
    "        iter += 1\n",
    "    \n",
    "    model_NewLLSF = W_s\n",
    "    return model_NewLLSF\n",
    "def softthres(W_t, lambda_):\n",
    "    return np.maximum(W_t-lambda_, 0) - np.maximum(-W_t-lambda_, 0)\n",
    "def ins_similarity(X, K):\n",
    "    A = squareform(pdist(X))\n",
    "    num_dim = A.shape[0]\n",
    "    for i in range(num_dim):\n",
    "        temp = A[i,:]\n",
    "        As = np.sort(temp)\n",
    "        temp = (temp <= As[K])\n",
    "        A[i,:] = temp\n",
    "    return A\n",
    "def label_similarity(Y,k):\n",
    "    m = Y.shape[1]\n",
    "    cos_sim = np.zeros((m, m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if i == j:\n",
    "                cos_sim[i, j] = 1.0\n",
    "            else:\n",
    "                cos_sim[i, j] = np.dot(Y[:, i], Y[:, j]) / (np.linalg.norm(Y[:, i]) * np.linalg.norm(Y[:, j]))\n",
    "    upper_triangle_indices = np.triu_indices(cos_sim.shape[1], k=1)\n",
    "    upper_triangle_values = cos_sim [upper_triangle_indices]\n",
    "    top_k_indices = np.argpartition(upper_triangle_values, -k)[-k:]\n",
    "    result_matrix = np.zeros_like(cos_sim)\n",
    "    result_matrix[upper_triangle_indices[0][top_k_indices], upper_triangle_indices[1][top_k_indices]] = upper_triangle_values[top_k_indices]\n",
    "    return result_matrix\n",
    "def gradL21(W):\n",
    "    num = W.shape[0]\n",
    "    D = np.zeros((num, num))\n",
    "    for i in range(num):\n",
    "        temp = np.linalg.norm(W[i,:], 2)\n",
    "        if temp != 0:\n",
    "            D[i,i] = 1/temp\n",
    "        else:\n",
    "            D[i,i] = 0\n",
    "    return D\n",
    "def TuneThreshold(output, target, bAllOne, metricIndex):\n",
    "    if bAllOne is None:\n",
    "        bAllOne = 1\n",
    "    elif metricIndex is None:\n",
    "        metricIndex = 3  # use MacroF\n",
    "    \n",
    "    num_train,num_class= target.shape\n",
    "    TotalNums = 50\n",
    "    min_score = 0\n",
    "    max_score = np.max(output)\n",
    "    step = (max_score - min_score) / TotalNums\n",
    "    tau_range = np.arange(min_score, max_score + step, step)  \n",
    "    \n",
    "    tau = np.zeros(num_class)  \n",
    "    currentResult = tau.copy()\n",
    "    for t in range(len(tau_range)):\n",
    "        threshold = tau_range[t]\n",
    "        if bAllOne == 1:  \n",
    "# set to only one threshold for all the class labels\n",
    "            thresholds = np.ones_like(output) * threshold\n",
    "            predict_target = np.where(output - thresholds >= 0, 1, 0)  \n",
    "            tempResult = evaluateOneMetric(target, predict_target, metricIndex)  \n",
    "            if tempResult > currentResult[0]:\n",
    "                currentResult[0] = tempResult\n",
    "                tau[0] = threshold\n",
    "        else:  \n",
    "            for c in range(num_class):\n",
    "                thresholds = np.ones(num_train) * threshold\n",
    "#                 pdb.set_trace()\n",
    "                predict_target_l = np.where(output[:,c] - thresholds >= 0, 1, 0)                \n",
    "                tempResult = evaluateF1(target[:,c], predict_target_l)\n",
    "                if tempResult > currentResult[c]:\n",
    "                    currentResult[c] = tempResult\n",
    "                    tau[c] = threshold        \n",
    "    if bAllOne == 1:\n",
    "        tau = np.ones(num_class) * tau[0]   \n",
    "    return tau, currentResult\n",
    "def evaluateF1(target, predict):\n",
    "    TP = target.dot(predict.T)\n",
    "    precision = TP / np.sum(predict != 0)\n",
    "    recall = TP / np.sum(target != 0)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "def evaluateOneMetric(target, predict_target, metric):\n",
    "    Result = 0\n",
    "    if metric == 1:\n",
    "        HammingScore = 1 - hamming_loss(predict_target, target)\n",
    "        Result = HammingScore\n",
    "    elif metric == 2 or metric == 3:\n",
    "        ExampleBasedAccuracy, _, _, ExampleBasedFmeasure = example_based_measure(target, predict_target)\n",
    "        if metric == 2:\n",
    "            Result =ExampleBasedAccuracy\n",
    "        else:\n",
    "            Result = ExampleBasedFmeasure\n",
    "    elif metric == 4 or metric == 5:\n",
    "        LabelBasedAccuracy, _, _, LabelBasedFmeasure = label_based_measure(target, predict_target)\n",
    "        if metric == 4:\n",
    "            Result = LabelBasedAccuracy\n",
    "        else:\n",
    "            Result = LabelBasedFmeasure\n",
    "    elif metric == 6:\n",
    "        SubsetAccuracy = subset_accuracy_evaluation(target, predict_target)\n",
    "        Result = SubsetAccuracy\n",
    "    return Result\n",
    "def label_based_measure(test_targets, predict_targets):\n",
    "    L, num_test = test_targets.shape\n",
    "    test_targets = test_targets.astype(np.float64)\n",
    "    predict_targets = predict_targets.astype(np.float64)\n",
    "    LabelBasedAccuracy = 0\n",
    "    LabelBasedPrecision = 0\n",
    "    LabelBasedRecall = 0\n",
    "    LabelBasedFmeasure = 0\n",
    "    for i in range(L):\n",
    "        intersection = np.dot(test_targets[i,:], predict_targets[i,:])\n",
    "        union = np.sum(np.logical_or(test_targets[i,:], predict_targets[i,:]))\n",
    "\n",
    "        if union != 0:\n",
    "            LabelBasedAccuracy += intersection / union\n",
    "\n",
    "        if np.sum(predict_targets[i,:]) != 0:\n",
    "            precision_i = intersection / np.sum(predict_targets[i,:])\n",
    "        else:\n",
    "            precision_i = 0\n",
    "\n",
    "        if np.sum(test_targets[i,:]) != 0:\n",
    "            recall_i = intersection / np.sum(test_targets[i,:])\n",
    "        else:\n",
    "            recall_i = 0\n",
    "\n",
    "        LabelBasedPrecision += precision_i\n",
    "        LabelBasedRecall += recall_i\n",
    "\n",
    "        if recall_i != 0 or precision_i != 0:\n",
    "            LabelBasedFmeasure += 2 * recall_i * precision_i / (recall_i + precision_i)\n",
    "\n",
    "    LabelBasedAccuracy /= L\n",
    "    LabelBasedPrecision /= L\n",
    "    LabelBasedRecall /= L\n",
    "    LabelBasedFmeasure /= L\n",
    "    return LabelBasedAccuracy, LabelBasedPrecision, LabelBasedRecall, LabelBasedFmeasure\n",
    "def example_based_measure(test_targets, predict_targets):\n",
    "\n",
    "    L, num_test = test_targets.shape\n",
    "    test_targets = np.double(test_targets == 1)\n",
    "    predict_targets = np.double(predict_targets == 1)\n",
    "\n",
    "    ExampleBasedAccuracy = 0\n",
    "    ExampleBasedPrecision = 0\n",
    "    ExampleBasedRecall = 0\n",
    "    ExampleBasedFmeasure = 0\n",
    "\n",
    "    for i in range(num_test):\n",
    "        intersection = np.dot(test_targets[:, i], predict_targets[:, i])\n",
    "        union = np.sum(np.logical_or(test_targets[:, i], predict_targets[:, i]))\n",
    "\n",
    "        if union != 0:\n",
    "            ExampleBasedAccuracy += intersection / union\n",
    "\n",
    "        if np.sum(predict_targets[:, i]) != 0:\n",
    "            precision_i = intersection / np.sum(predict_targets[:, i])\n",
    "        else:\n",
    "            precision_i = 0\n",
    "\n",
    "        if np.sum(test_targets[:, i]) != 0:\n",
    "            recall_i = intersection / np.sum(test_targets[:, i])\n",
    "        else:\n",
    "            recall_i = 0\n",
    "\n",
    "        ExampleBasedPrecision += precision_i\n",
    "        ExampleBasedRecall += recall_i\n",
    "\n",
    "        if recall_i != 0 or precision_i != 0:\n",
    "            ExampleBasedFmeasure += 2 * recall_i * precision_i / (recall_i + precision_i)\n",
    "    ExampleBasedAccuracy /= num_test\n",
    "    ExampleBasedPrecision /= num_test\n",
    "    ExampleBasedRecall /= num_test\n",
    "    ExampleBasedFmeasure /= num_test\n",
    "    return ExampleBasedAccuracy, ExampleBasedPrecision, ExampleBasedRecall, ExampleBasedFmeasure\n",
    "def subset_accuracy_evaluation(test_target, predict_target):\n",
    "    _, num_test = test_target.shape\n",
    "    count = 0\n",
    "    for i in range(num_test):\n",
    "        if np.array_equal(test_target[:,i], predict_target[:,i]):\n",
    "            count += 1\n",
    "    SubsetAccuracy = count / num_test\n",
    "    return SubsetAccuracy\n",
    "def Predict(Outputs, tau):\n",
    "    predict_target = np.zeros_like(Outputs)  # Create a zero matrix with the same shape as Outputs\n",
    "    num_class = Outputs.shape[1]  # Get the number of class labels\n",
    "    for c in range(num_class):\n",
    "        predict_target[:, c] = Outputs[:, c] >= tau[c]  # Copy values satisfying the tau condition to the final prediction matrix\n",
    "    return predict_target\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d2b8e",
   "metadata": {},
   "source": [
    "# Sampling of LSDMLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1c1da0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T07:35:29.518022Z",
     "start_time": "2023-11-07T07:35:29.487955Z"
    },
    "code_folding": [
     6,
     23,
     28,
     31,
     47,
     51,
     72
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import random\n",
    "import pdb\n",
    "import pickle\n",
    "def CalMinkowski(df,weights,k,p1):\n",
    "    n = df.shape[0]  \n",
    "    distances = np.zeros((n, n))  \n",
    "    data=np.array(df)\n",
    "    if p1==np.inf:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                distances[i, j]=np.max(weights* np.abs(data[i] - data[j]))               \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for j in range(i, n): \n",
    "                distances[i, j] = np.power(np.sum(weights * np.abs(data[i] - data[j])**p1), 1/p1)\n",
    "        distances = distances + distances.T - np.diag(distances.diagonal())\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "    sorted_indices = sorted_indices[:, ::1]\n",
    "    topk_indices= sorted_indices[:, :k]\n",
    "    return topk_indices\n",
    "def assign_weights(arr):\n",
    "    arr =np.abs(arr)\n",
    "    weights = np.exp(arr - np.max(arr)) / np.sum(np.exp(arr - np.max(arr)))\n",
    "#     weights =np.abs(arr)\n",
    "    return weights\n",
    "def weighted_minkowski_distance(v1, v2, w):\n",
    "    distance = np.power(np.sum(w * np.abs(v1 - v2)**2), 1/2)\n",
    "    return distance\n",
    "def Imbalance(X,y):\n",
    "    countmatrix=[]\n",
    "    for i in range(y.shape[1]):\n",
    "        count0=0\n",
    "        count1=0\n",
    "        for j in range(y.shape[0]):\n",
    "            if y[j,i]==1:\n",
    "                count1+=1\n",
    "            else:\n",
    "                count0+=1\n",
    "        countmatrix.append(count1)\n",
    "    maxcount=max(countmatrix)\n",
    "    ImbalanceRatioMatrix=[maxcount/i for i in countmatrix]\n",
    "    MaxIR=max(ImbalanceRatioMatrix)\n",
    "    MeanIR=sum(ImbalanceRatioMatrix)/len(ImbalanceRatioMatrix)\n",
    "    return ImbalanceRatioMatrix,MeanIR,countmatrix\n",
    "def CalcuNN(df1,n_neighbor):\n",
    "    nbs=NearestNeighbors(n_neighbors=n_neighbor,metric='euclidean',algorithm='kd_tree').fit(df1)\n",
    "    euclidean,indices= nbs.kneighbors(df1)\n",
    "    return euclidean,indices\n",
    "def Labeltype(X,y):   \n",
    "    ImbalanceRatioMatrix,MeanIR,_=Imbalance(X,y)\n",
    "    DifferenceImbalanceRatioMatrix=[i-MeanIR for i in ImbalanceRatioMatrix]\n",
    "    MinLabelIndex=[]\n",
    "    MajLabelIndex=[]\n",
    "    count=0\n",
    "    for i in (DifferenceImbalanceRatioMatrix):\n",
    "        if i>0:\n",
    "            MinLabelIndex.append(count)\n",
    "        else:\n",
    "            MajLabelIndex.append(count)\n",
    "        count+=1\n",
    "    MinLabelName=[]\n",
    "    MajLabelName=[]\n",
    "    for i in MinLabelIndex:\n",
    "        MinLabelName.append(label_names[i][0])\n",
    "    for i in MajLabelIndex:\n",
    "        MajLabelName.append(label_names[i][0])\n",
    "    MinLabeldic=dict(zip(MinLabelIndex,MinLabelName))\n",
    "    MajLabeldic=dict(zip(MajLabelIndex,MajLabelName))\n",
    "    return MinLabeldic,MajLabeldic\n",
    "def SPECIAL(df1, df2, W, sp,dataset_name):\n",
    "    ImrMatrix=ImR(X,y)\n",
    "    n_neighbors = 5\n",
    "    p=2\n",
    "    cos_sim = label_similarity(np.array(df2),10)\n",
    "    non_zero_indices = [np.where(row != 0)[0].tolist() for row in cos_sim]\n",
    "    row_sums = cos_sim.sum(axis=1)\n",
    "    normalized_label_weight = cos_sim / row_sums[:, np.newaxis]\n",
    "    MinLabeldic, MajLabeldic = Labeltype(np.array(df1), np.array(df2))\n",
    "    ImbalanceRatioMatrix, MeanIR, countmatrix = Imbalance(np.array(df1), np.array(df2))\n",
    "    MinLabelindex = list(MinLabeldic.keys())\n",
    "    C = np.zeros((df1.shape[0], len(MinLabelindex)))\n",
    "    C_hat=np.zeros((df1.shape[0], len(MinLabelindex))) \n",
    "    dataset_name = os.path.join(os.getcwd(), \"rcvpara\", dataset_name)\n",
    "#     pdb.set_trace()\n",
    "    if os.path.exists(dataset_name):\n",
    "        with open(dataset_name, 'rb') as file:\n",
    "            indices_dict = pickle.load(file)\n",
    "        ifcalnn=False\n",
    "    else:\n",
    "        indices_dict = {}\n",
    "        ifcalnn=True\n",
    "    for tail_label in MinLabelindex:\n",
    "        if ImrMatrix[tail_label]>10:\n",
    "            continue\n",
    "        all_relevant=non_zero_indices[tail_label]\n",
    "        sub_index = np.where(df2[MinLabeldic[tail_label]] == 1)[0]\n",
    "        idx = MinLabelindex.index(tail_label)\n",
    "        \n",
    "        W_tail_label = W[:, tail_label]\n",
    "        sorted_indices = np.argsort(W_tail_label)\n",
    "        sorted_column = W_tail_label[sorted_indices]\n",
    "        featureWeight = assign_weights(W_tail_label)\n",
    "        if ifcalnn==True:\n",
    "            indices = CalMinkowski(df1, featureWeight, n_neighbors + 1, p)\n",
    "#             euclidean,indices2=CalcuNN(df1,n_neighbors + 1)\n",
    "            indices_dict[tail_label] = indices\n",
    "        elif ifcalnn==False:\n",
    "            indices=indices_dict[tail_label]\n",
    "        for i in range(df1.shape[0]):\n",
    "            if df2.iloc[i,tail_label]==0:\n",
    "                continue\n",
    "            count=0\n",
    "            for j in indices[i, 1:]:\n",
    "                if df2.iloc[i,tail_label]==df2.iloc[j,tail_label]:\n",
    "                    count +=1\n",
    "            C[i, MinLabelindex.index(tail_label)] = count / n_neighbors\n",
    "            count1list=[]\n",
    "            if all_relevant:   \n",
    "                for k in all_relevant:\n",
    "                    count1=0\n",
    "                    for j in indices[i, :]:\n",
    "                        if df2.iloc[j, k] == 1:\n",
    "                            count1+= 1\n",
    "                    count1list.append(count1)\n",
    "                C_hat[i, MinLabelindex.index(tail_label)]=max(count1list) / n_neighbors      \n",
    "    Ins_Weight=np.zeros(df1.shape[0])  \n",
    "    tem = np.zeros([df1.shape[0], len(MinLabelindex)])\n",
    "    tem_hat = np.zeros([df1.shape[0], len(MinLabelindex)])\n",
    "\n",
    "\n",
    "    for j in range(len(MinLabelindex)):\n",
    "        SumC = 0.0\n",
    "        sum_C_1 = 0.0\n",
    "        c = 0\n",
    "        c_1 = 0\n",
    "\n",
    "        for i in range(df1.shape[0]):\n",
    "            if C[i, j] < 1 and C[i, j] != 0:\n",
    "                SumC += C[i, j]\n",
    "                c += 1\n",
    "            if C_hat[i, j] != 0:\n",
    "                sum_C_1 += C_hat[i, j]\n",
    "                c_1 += 1\n",
    "\n",
    "            if SumC != 0.0 and c != 0:\n",
    "                if C[i, j] < 1 and C[i, j] != 0:\n",
    "                    tem[i, j] = C[i, j] / SumC\n",
    "            else:\n",
    "                tem[i, j] = 0\n",
    "\n",
    "            if sum_C_1 != 0.0 and c_1 != 0:\n",
    "                if C_hat[i, j] != 0:\n",
    "                    tem_hat[i, j] = C_hat[i, j] / sum_C_1\n",
    "            else:\n",
    "                tem_hat[i, j] = 0\n",
    "\n",
    "    # check完毕，C被tem取代\n",
    "    SumW = 0\n",
    "    for i in range(df1.shape[0]):\n",
    "        for j in range(len(MinLabelindex)):\n",
    "            if tem[i, j] != 0:\n",
    "                Ins_Weight[i] += tem[i, j] + tem_hat[i, j]\n",
    "        SumW += Ins_Weight[i]\n",
    "    non_zero_elements = []\n",
    "\n",
    "    for row in tem:\n",
    "        for element in row:\n",
    "            if element != 0:\n",
    "                non_zero_elements.append(element)\n",
    "                \n",
    "           \n",
    "    n_sample = int(df1.shape[0] * sp)\n",
    "    new_X = np.zeros((n_sample, df1.shape[1]))\n",
    "    target = np.zeros((n_sample, df2.shape[1]))\n",
    "    count = 0 \n",
    "    \n",
    "    while count < n_sample:  # 修正此处循环变量为count\n",
    "        random_count=np.random.random()*SumW\n",
    "        seed=0\n",
    "        s=0\n",
    "        for k in range(len(Ins_Weight)):\n",
    "            s+=Ins_Weight[k]\n",
    "            if(random_count<=s):\n",
    "                seed=k\n",
    "                break    \n",
    "        seedtype = np.where(np.array(df2.iloc[seed]) == 1)[0]\n",
    "        set1 = set(seedtype)\n",
    "        set2 = set(MinLabelindex)\n",
    "        intersection = set1.intersection(set2)\n",
    "        intersection_list = list(intersection)\n",
    "        if not intersection_list:\n",
    "            continue  # 当 intersection_list 为空时，跳过当前循环\n",
    "        select_index=np.random.choice(intersection_list)\n",
    "        reference = np.random.choice(indices_dict[select_index][seed, 1:])\n",
    "        all_point = indices_dict[select_index][seed, :]\n",
    "        nn_df = df2[df2.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)   \n",
    "        for j in range(df1.shape[1]):\n",
    "            ratio = np.random.random()\n",
    "#             pdb.set_trace()\n",
    "            if feature_names[j][1] == 'NUMERIC':\n",
    "                new_X[count, j] = df1.iloc[seed, j] + ratio * (df1.iloc[reference, j] - df1.iloc[seed, j])\n",
    "            elif feature_names[j][1] == ['YES', 'NO'] or feature_names[j][1] == ['0', '1']:\n",
    "                rmd = np.random.choice([True, False])\n",
    "                if rmd:\n",
    "                    new_X[count, j] = df1.iloc[seed, j]\n",
    "                else:\n",
    "                    new_X[count, j] = df1.iloc[reference, j]\n",
    "            else:\n",
    "                new_X[count, j] = df1.iloc[seed, j]    \n",
    "        target[count] = np.array([1 if val>=((len(all_point))/2) else 0 for val in ser])\n",
    "#         for j in range(df2.shape[1]):\n",
    "#             if df2.iloc[seed, j] == df2.iloc[reference, j]:\n",
    "#                 target[count, j]=df2.iloc[seed, j]\n",
    "#             else:\n",
    "#                 featureWeight = assign_weights(W[:, j])\n",
    "#                 distance1 = weighted_minkowski_distance(np.array(df1.iloc[seed, :]), new_X[count, :], featureWeight)\n",
    "#                 distance2 = weighted_minkowski_distance(np.array(df1.iloc[reference, :]), new_X[count, :], featureWeight)\n",
    "#                 if distance1 <= distance2:\n",
    "#                     target[count, j] = df2.iloc[seed, j]\n",
    "#                 else:\n",
    "#                     target[count, j] = df2.iloc[reference, j]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # 创建新的数据框并返回结果\n",
    "#     new_X = new_X[~np.all(new_X == 0, axis=1)]\n",
    "#     target= target[~np.all(target == 0, axis=1)]\n",
    "    new_X = pd.DataFrame(new_X, columns=[x[0] for x in feature_names])\n",
    "    target = pd.DataFrame(target, columns=[y[0] for y in label_names])\n",
    "    ML_SOL_new_X = pd.concat([df1, new_X], axis=0).reset_index(drop=True)\n",
    "    ML_SOL_target = pd.concat([df2, target], axis=0).reset_index(drop=True)\n",
    "    if ifcalnn==True:\n",
    "        with open(dataset_name, 'wb') as file:\n",
    "            pickle.dump(indices_dict, file)\n",
    "    return ML_SOL_new_X, ML_SOL_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5c2c083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T07:35:33.027512Z",
     "start_time": "2023-11-07T07:35:32.972419Z"
    },
    "code_folding": [
     20,
     41,
     50
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import sklearn.metrics as metrics\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from skmultilearn.ensemble import RakelD\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device('cuda')\n",
    "def FeatureSelect(p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        if feature_names[1][1]=='NUMERIC':\n",
    "            featurecount=int(X.shape[1]*p)\n",
    "            column_variances = np.var(X.toarray(), axis=0)\n",
    "            sorted_indices = column_variances.argsort()[::-1]\n",
    "            Selectfeatureindex = sorted_indices[:featurecount]\n",
    "            Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "            featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "            new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "            new_featurename=[feature_names[i] for i in Selectfeatureindex]          \n",
    "        else:\n",
    "            featurecount=int(X.shape[1]*p)\n",
    "            Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "            Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "            featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "            new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "            new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def LabelSelect():\n",
    "    b=[]\n",
    "    new_labelname=[i for i in label_names]\n",
    "    for i in range(y.shape[1]):\n",
    "        if y[:,i].sum()<=10:\n",
    "            b.append(i)\n",
    "            new_labelname.remove(label_names[i])\n",
    "    new_y=np.delete(y.toarray(),b,axis=1)\n",
    "    return new_y,new_labelname \n",
    "# traning\n",
    "def training(classiferindex,index,samplingcount,dataset_name,*args):\n",
    "    Randomlist=[7,10,19,30,23]\n",
    "    Macro=[]\n",
    "    Micro=[]\n",
    "    Hamming_loss=[]\n",
    "    rankingloss=[]\n",
    "    macropr=[]\n",
    "    macroaucroc=[]\n",
    "    Avgpr=[]\n",
    "    for i in Randomlist:  \n",
    "        k_fold = IterativeStratification(n_splits=2,order=1,random_state=i)\n",
    "        j=0\n",
    "        for train,test in k_fold.split(X,y):\n",
    "            j+=1\n",
    "            if classiferindex==1:\n",
    "                classifier =BinaryRelevance(\n",
    "                    classifier = DecisionTreeClassifier(random_state=41),\n",
    "                    require_dense = [False, True]\n",
    "                )\n",
    "            elif classiferindex==2:\n",
    "                classifier =MLkNN(k=10)\n",
    "            elif classiferindex==3:\n",
    "                classifier =ClassifierChain(\n",
    "                    classifier = DecisionTreeClassifier(random_state=20),\n",
    "                    require_dense = [False, True]\n",
    "                )\n",
    "            elif classiferindex==4:\n",
    "                classifier = RakelD(\n",
    "                    base_classifier=DecisionTreeClassifier(random_state=20),\n",
    "                    base_classifier_require_dense=[False, True],\n",
    "                    labelset_size=3\n",
    "                )\n",
    "            elif classiferindex==5:\n",
    "                classifier=COCOA()\n",
    "#                 classifier=ECC()\n",
    "            \n",
    "            if(index==1):\n",
    "                X1,y1=X[train],y[train]\n",
    "            else: \n",
    "                dfx=pd.DataFrame(X[train],columns=[x[0] for x in feature_names])\n",
    "                dfy=pd.DataFrame(y[train],columns=[x[0] for x in label_names])\n",
    "                if (index==2):\n",
    "                    new_X,new_y=MLSMOTE(dfx,dfy,5,\"Union\")\n",
    "                elif (index==3):\n",
    "                    new_X,new_y=MLSMOTE(dfx,dfy,5,\"Ranking\")\n",
    "                elif (index==4):\n",
    "                    new_X,new_y=MLSOL(dfx,dfy,samplingcount)\n",
    "                elif (index==5):\n",
    "                    new_X,new_y=MLROS(dfx,dfy,samplingcount)\n",
    "                elif (index==6):\n",
    "                    new_X,new_y=MLRUS(dfx,dfy,samplingcount)\n",
    "                elif(index==7):\n",
    "                    new_X1,new_y1=REMEDIAL(dfx,dfy)\n",
    "                    new_X,new_y=MLSMOTE(new_X1,new_y1,5,\"R\")\n",
    "                elif(index==8):\n",
    "                    new_X1,new_y1=REMEDIAL(dfx,dfy)\n",
    "                    new_X,new_y=MLROS(dfx,dfy,0.1)\n",
    "                elif(index==9):\n",
    "                    new_X,new_y=MLBMOTE(dfx,dfy, 0.05, 0.05, 2, feature_names)\n",
    "                    print(new_X.shape[0]-dfx.shape[0])                 \n",
    "                else:\n",
    "                    X1, y1 = X[train], y[train]\n",
    "                    if j==1:\n",
    "                        W = CLML(X1, y1, optmParameter)\n",
    "                        W1=W\n",
    "                        non_zero_counts = np.count_nonzero(W, axis=0)\n",
    "                        dataset_name=dataset_name+str(j)\n",
    "                        new_X, new_y = SPECIAL1(dfx, dfy, W,samplingcount,dataset_name)\n",
    "                    else:\n",
    "                        dataset_name=dataset_name+str(j)\n",
    "                        new_X, new_y = SPECIAL1(dfx, dfy, W1,samplingcount,dataset_name)\n",
    "                X1,y1=np.array(new_X),np.array(new_y)\n",
    "            classifier.fit(X1,y1)\n",
    "            X2,y2=X[test],y[test]\n",
    "            ypred = classifier.predict(X2)\n",
    "            yprob=classifier.predict_proba(X2)\n",
    "            if classiferindex==1 or classiferindex==2 or classiferindex==3 or classiferindex==4:\n",
    "                yprob=yprob.toarray()\n",
    "            Macro.append(metrics.f1_score(y2, ypred,average='macro'))\n",
    "            Micro.append(metrics.f1_score(y2, ypred,average='micro'))\n",
    "            rankingloss.append(metrics.label_ranking_loss(y2,yprob))                     \n",
    "            macropr.append(metrics.average_precision_score(y2,yprob,average='macro'))\n",
    "            macroaucroc.append(metrics.roc_auc_score(y2,yprob,average='macro'))  \n",
    "            Avgpr.append(metrics.average_precision_score(y2,yprob,average='samples'))\n",
    "            Hamming_loss.append(metrics.hamming_loss(y2, ypred))  \n",
    "    Avgpr=[a for a in Avgpr if a==a]\n",
    "    \n",
    "    MacroF=sum(Macro)/len(Macro)\n",
    "#     print(Macro)\n",
    "    MicroF=sum(Micro)/len(Micro)\n",
    "    MacroAUCROC=sum(macroaucroc)/len(macroaucroc)\n",
    "#     print(macroaucroc)\n",
    "    MacroAUCPR=sum(macropr)/len(macropr)\n",
    "    RankLoss=sum(rankingloss)/len(rankingloss)\n",
    "    hamming=sum(Hamming_loss)/len(Hamming_loss)\n",
    "    if len(Avgpr)!=0:\n",
    "        Avgprecison=sum(Avgpr)/len(Avgpr)\n",
    "    else:\n",
    "        Avgprecison=0\n",
    "    res2=(sum((i-MacroF)**2 for i in Macro)/len(Macro))**0.5\n",
    "    res3=(sum((i-RankLoss)**2 for i in rankingloss)/len(rankingloss))**0.5\n",
    "    res4=(sum((i-MacroAUCROC)**2 for i in macroaucroc)/len(macroaucroc))**0.5\n",
    "    res6=(sum((i-MacroAUCPR)**2 for i in macropr)/len(macropr))**0.5\n",
    "    if len(Avgpr)!=0:\n",
    "        res7=(sum((i-Avgprecison)**2 for i in Avgpr)/len(Avgpr))**0.5\n",
    "    else:\n",
    "        res7=0\n",
    "    MacroF=round(MacroF,4)\n",
    "    MicroF=round(MicroF,4)\n",
    "    MacroAUCROC=round(MacroAUCROC,4)\n",
    "    MacroAUCPR=round(MacroAUCPR,4)\n",
    "    RankLoss=round(RankLoss,4)\n",
    "    Avgprecison=round(Avgprecison,4)\n",
    "    hamming=round(hamming,4)\n",
    "    res2=round(res2,4)\n",
    "    res4=round(res4,4)\n",
    "    res6=round(res6,4)\n",
    "    res3=round(res3,4)\n",
    "    res7=round(res7,4)\n",
    "#     print(MacroF,MacroAUCROC,MacroAUCPR,RankLoss,res2,res4,res6,res3,res7)\n",
    "    return MacroF,MacroAUCROC,MacroAUCPR,RankLoss,Avgprecison,res2,res4,res6,res3,res7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae92c7",
   "metadata": {},
   "source": [
    "# BaseLine and Compared sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3b87672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:49:32.479714Z",
     "start_time": "2023-11-06T07:26:41.494159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "yahoo-Business1\n",
      "classifierid: 1\n",
      "0.1361 0.5745 0.0454 0.2262 0.0065 0.0049 0.002 0.0032\n",
      "0.1352 0.5744 0.0449 0.2263 0.0076 0.0052 0.0028 0.0022\n",
      "0.135 0.5743 0.0448 0.2262 0.0062 0.004 0.0021 0.0029\n",
      "0.13 0.5712 0.0435 0.2272 0.0066 0.0046 0.0027 0.0031\n",
      "0.1321 0.5723 0.0444 0.2264 0.0041 0.003 0.0014 0.0024\n",
      "0.132 0.5705 0.0444 0.2272 0.0067 0.0043 0.0022 0.0035\n",
      "0.1171 0.5737 0.0472 0.2255 0.0065 0.003 0.0026 0.0024\n",
      "0.1321 0.5723 0.0444 0.2264 0.0041 0.003 0.0014 0.0024\n",
      "classifierid: 2\n",
      "0.1187 0.6326 0.1132 0.0556 0.0104 0.0072 0.0044 0.004\n",
      "0.129 0.6397 0.1141 0.057 0.0118 0.0044 0.0053 0.0038\n",
      "0.1282 0.6388 0.1153 0.0575 0.0128 0.0043 0.0054 0.0042\n",
      "0.1629 0.6405 0.1147 0.0566 0.0099 0.0089 0.0079 0.0035\n",
      "0.129 0.6305 0.1076 0.0581 0.0094 0.0065 0.0073 0.0034\n",
      "0.1106 0.6284 0.1112 0.0559 0.0126 0.0055 0.0046 0.0036\n",
      "0.1041 0.6354 0.1121 0.0582 0.0086 0.0057 0.0052 0.0031\n",
      "0.129 0.6305 0.1076 0.0581 0.0094 0.0065 0.0073 0.0034\n",
      "classifierid: 3\n",
      "0.1294 0.5712 0.0427 0.2274 0.0061 0.0047 0.0018 0.0024\n",
      "0.1254 0.5702 0.0417 0.2277 0.0054 0.0054 0.0013 0.0024\n",
      "0.129 0.5725 0.0426 0.2274 0.0039 0.0047 0.0013 0.0031\n",
      "0.1252 0.5694 0.0415 0.2284 0.0054 0.0043 0.0019 0.0015\n",
      "0.1232 0.5666 0.0412 0.2289 0.0062 0.0048 0.0015 0.0028\n",
      "0.1248 0.5676 0.0412 0.2296 0.0075 0.0052 0.0022 0.0029\n",
      "0.1151 0.5614 0.0423 0.2311 0.006 0.0055 0.002 0.0025\n",
      "0.1232 0.5666 0.0412 0.2289 0.0062 0.0048 0.0015 0.0028\n",
      "classifierid: 4\n",
      "0.1326 0.5681 0.0447 0.2273 0.0082 0.0044 0.0022 0.0026\n",
      "0.1322 0.5682 0.0439 0.2272 0.0047 0.003 0.002 0.003\n",
      "0.1328 0.5679 0.0446 0.2277 0.0071 0.0037 0.0016 0.0022\n",
      "0.1338 0.5697 0.0444 0.2275 0.0051 0.0044 0.0016 0.004\n",
      "0.1366 0.5702 0.0455 0.2274 0.008 0.0039 0.0015 0.0031\n",
      "0.1319 0.566 0.0448 0.2291 0.0096 0.0055 0.0023 0.0042\n",
      "0.1213 0.5705 0.0486 0.2274 0.0086 0.0046 0.0024 0.0023\n",
      "0.1359 0.5696 0.0455 0.2276 0.0092 0.0036 0.003 0.0036\n",
      "classifierid: 5\n",
      "0.197 0.7116 0.1285 0.0446 0.0077 0.0093 0.0064 0.0008\n",
      "0.1921 0.7096 0.1234 0.0451 0.0058 0.0089 0.0062 0.0008\n",
      "0.1964 0.71 0.1285 0.0449 0.0062 0.0097 0.0058 0.0007\n",
      "0.2011 0.7109 0.1335 0.0449 0.0088 0.0091 0.0082 0.0008\n",
      "0.1933 0.7055 0.1257 0.0449 0.0112 0.0096 0.0075 0.0008\n",
      "0.1949 0.7093 0.1282 0.0446 0.0077 0.0092 0.007 0.001\n",
      "0.1902 0.7069 0.1244 0.0448 0.0089 0.0099 0.0057 0.0009\n",
      "0.1933 0.7055 0.1257 0.0449 0.0112 0.0096 0.0075 0.0008\n",
      "----------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------\n",
      "yahoo-Arts1\n",
      "classifierid: 1\n",
      "0.1495 0.5537 0.0844 0.5963 0.0041 0.0023 0.0012 0.0028\n",
      "0.1491 0.5532 0.0843 0.5936 0.0045 0.0029 0.0013 0.0071\n",
      "0.1475 0.5523 0.0843 0.5954 0.0056 0.0045 0.0015 0.007\n",
      "0.1481 0.5526 0.0839 0.5957 0.0035 0.0028 0.001 0.0041\n",
      "0.1465 0.551 0.0835 0.5973 0.0022 0.0018 0.0006 0.0043\n",
      "0.1512 0.5541 0.0852 0.5924 0.0052 0.0034 0.0013 0.0066\n",
      "0.128 0.5534 0.0875 0.5925 0.0044 0.0026 0.001 0.0039\n",
      "0.1465 0.551 0.0835 0.5973 0.0022 0.0018 0.0006 0.0043\n",
      "classifierid: 2\n",
      "0.1082 0.6104 0.1386 0.17 0.0075 0.0041 0.0032 0.004\n",
      "0.1162 0.612 0.1362 0.1759 0.0061 0.0033 0.0033 0.0028\n",
      "0.1169 0.6148 0.1396 0.1796 0.0082 0.0037 0.0036 0.0039\n",
      "0.1564 0.6111 0.1387 0.173 0.0057 0.0047 0.004 0.0037\n",
      "0.1101 0.6084 0.1334 0.174 0.0072 0.0033 0.0041 0.0034\n",
      "0.1083 0.6103 0.1391 0.1691 0.0087 0.0039 0.0032 0.0031\n",
      "0.0937 0.6098 0.1376 0.1835 0.0067 0.0039 0.0029 0.0029\n",
      "0.1101 0.6084 0.1334 0.174 0.0072 0.0033 0.0041 0.0034\n",
      "classifierid: 3\n",
      "0.1448 0.5512 0.0809 0.6044 0.0045 0.0035 0.0011 0.0042\n",
      "0.1411 0.5488 0.0801 0.6024 0.0051 0.004 0.0016 0.0036\n",
      "0.1416 0.5488 0.0803 0.6076 0.0057 0.0047 0.0017 0.0046\n",
      "0.1455 0.5517 0.0812 0.6021 0.0042 0.0032 0.0008 0.0041\n",
      "0.1408 0.5484 0.0802 0.6056 0.0043 0.0029 0.0016 0.0067\n",
      "0.1439 0.5507 0.0811 0.6024 0.0061 0.0046 0.002 0.0061\n",
      "0.1323 0.5471 0.0823 0.6191 0.0061 0.0037 0.0014 0.0065\n",
      "0.1408 0.5484 0.0802 0.6056 0.0043 0.0029 0.0016 0.0067\n",
      "classifierid: 4\n",
      "0.1507 0.5523 0.0853 0.5991 0.0042 0.0022 0.0014 0.0048\n",
      "0.148 0.5512 0.0853 0.5958 0.0041 0.0023 0.001 0.0066\n",
      "0.1496 0.5519 0.0858 0.5979 0.0042 0.0026 0.0013 0.0074\n",
      "0.1524 0.5535 0.086 0.5968 0.0055 0.0034 0.0015 0.0069\n",
      "0.151 0.5526 0.0857 0.5985 0.0067 0.0046 0.0016 0.0068\n",
      "0.1485 0.5517 0.0855 0.5982 0.0038 0.0022 0.0019 0.0052\n",
      "0.1296 0.5526 0.0881 0.5972 0.0063 0.0036 0.0017 0.0056\n",
      "0.1487 0.5513 0.085 0.5979 0.0056 0.0036 0.0013 0.0064\n",
      "classifierid: 5\n",
      "0.2019 0.6855 0.1639 0.1408 0.0068 0.0064 0.0036 0.0025\n",
      "0.2036 0.6819 0.1608 0.143 0.0058 0.006 0.0039 0.0028\n",
      "0.2033 0.6835 0.1623 0.1441 0.0066 0.0053 0.0035 0.0028\n",
      "0.2089 0.6834 0.1645 0.1419 0.0057 0.0064 0.0039 0.0026\n",
      "0.2004 0.6805 0.1605 0.1437 0.0049 0.0062 0.0035 0.0025\n",
      "0.2023 0.6848 0.1637 0.1411 0.0062 0.0062 0.0039 0.0026\n",
      "0.1929 0.6806 0.1598 0.1444 0.0084 0.0051 0.0046 0.0026\n",
      "0.2004 0.6805 0.1605 0.1437 0.0049 0.0062 0.0035 0.0025\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "path_to_arff_files = [\"emotions.arff\",\"scene.arff\",\"yeast.arff\", \"rcv1subset1.arff\",\"rcv1subset2.arff\",\"rcv1subset3.arff\",\"Corel5k.arff\",\"yahoo-Business1\",\"yahoo-Arts1.arff\"]\n",
    "label_counts = [6, 6,14,101,101,101,374,28,25]\n",
    "select_feature=[1,1,1,0.01,0.01,0.01,1,0.01,0.01]\n",
    "data_specific_sampling=[0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "for i, path_to_arff_file in enumerate(path_to_arff_files):\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[i],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(select_feature[i])\n",
    "    y,label_names=LabelSelect()\n",
    "    dataset_name = path_to_arff_file.split(\".\")[0] \n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    print(dataset_name)\n",
    "    for c_idx in range(1,6):\n",
    "        print (f\"classifierid: {c_idx}\")\n",
    "        for m_idx in range(1, 9):\n",
    "            if m_idx==6:\n",
    "                sampling= data_specific_sampling2\n",
    "            else:\n",
    "                sampling= data_specific_sampling\n",
    "            if m_idx<=3 or m_idx==7 or m_idx==8:\n",
    "                MacroF,MacroAUCROC,MacroAUCPR,RankLoss,Avgprecison,res2,res4,res6,res3,res7=training(c_idx,m_idx,sp,dataset_name)\n",
    "#                 print (f\"sampling_method: {m_idx}\")\n",
    "#                 print(f\"MacroF: {MacroF}, MacroAUCROC: {MacroAUCROC}, MacroAUCPR: {MacroAUCPR},RankLoss: {RankLoss},res2: {res2}, res4: {res4}, res6: {res6}, res3: {res3}\")\n",
    "                print(MacroF, MacroAUCROC, MacroAUCPR, RankLoss, res2, res4, res6, res3)\n",
    "                continue\n",
    "            else:\n",
    "                MacroF_list = []\n",
    "                MacroAUCROC_list = []\n",
    "                MacroAUCPR_list = []\n",
    "                RankLoss_list = []\n",
    "                res2_list = []\n",
    "                res4_list = []\n",
    "                res6_list = []\n",
    "                res3_list = []\n",
    "#                 print (f\"sampling_method: {m_idx}\")\n",
    "                for sp in sampling:\n",
    "#                     print (f\"sampling_count: {sp}\")\n",
    "                    MacroF,MacroAUCROC,MacroAUCPR,RankLoss,Avgprecison,res2,res4,res6,res3,res7=training(c_idx,m_idx,sp,dataset_name)\n",
    "                    MacroF_list.append(MacroF)\n",
    "                    MacroAUCROC_list.append(MacroAUCROC)\n",
    "                    MacroAUCPR_list.append(MacroAUCPR)\n",
    "                    RankLoss_list.append(RankLoss)\n",
    "                    res2_list.append(res2)\n",
    "                    res4_list.append(res4)\n",
    "                    res6_list.append(res6)\n",
    "                    res3_list.append(res3)\n",
    "                max_MacroF = max(MacroF_list)\n",
    "                max_MacroAUCROC = max(MacroAUCROC_list)\n",
    "                max_MacroAUCPR = max(MacroAUCPR_list)\n",
    "                min_RankLoss = min(RankLoss_list)\n",
    "                res2_max = res2_list[MacroF_list.index(max_MacroF)]\n",
    "                res4_max = res4_list[MacroAUCROC_list.index(max_MacroAUCROC)]\n",
    "                res6_max = res6_list[MacroAUCPR_list.index(max_MacroAUCPR)]\n",
    "                res3_min = res3_list[RankLoss_list.index(min_RankLoss)]\n",
    "                print(max_MacroF, max_MacroAUCROC, max_MacroAUCPR, min_RankLoss, res2_max, res4_max, res6_max, res3_min)\n",
    "#                 print(f\"MacroF: {max(MacroF_list)}, MacroAUCROC: {max(MacroAUCROC_list)}, MacroAUCPR: {max(MacroAUCPR_list)}, RankLoss: {min(RankLoss_list)},res2: {res2_list[MacroF_list.index(max(MacroF_list))]}, res4: {res4_list[MacroAUCROC_list.index(max(MacroAUCROC_list))]}, res6: {res6_list[MacroAUCPR_list.index(max(MacroAUCPR_list))]}, res3: {res3_list[RankLoss_list.index(min(RankLoss_list))]}\")\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d74497",
   "metadata": {},
   "source": [
    "# Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4251af",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-07T07:47:54.488Z"
    },
    "code_folding": [
     7
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene\n",
      "classifierid: 1\n",
      "0.5945 0.7534 0.441 0.447 0.0067 0.0102 0.0072 0.0176\n",
      "classifierid: 2\n",
      "0.7323 0.926 0.775 0.0868 0.0073 0.0028 0.0045 0.0031\n",
      "classifierid: 3\n",
      "0.5979 0.7555 0.4448 0.4248 0.013 0.0087 0.0122 0.015\n",
      "classifierid: 4\n"
     ]
    }
   ],
   "source": [
    "paracombine=[[2**(0),2**(-4)],[2**(0),2**(-2)],[2**(1),2**(-2)],[2**(2),2**(0)],[2**(3),2**(0)],[2**(3),2**(1)],[2**(4),2**(0)],[2**(4),2**(1)]]\n",
    "# 'alpha'=lamda1,'beta'=lamda2,not give all parameter combinations\n",
    "path_to_arff_files = [\"emotions.arff\",\"scene.arff\",\"yeast.arff\", \"rcv1subset1.arff\",\"rcv1subset2.arff\",\"rcv1subset3.arff\",\"Corel5k.arff\",\"yahoo-Business1\",\"yahoo-Arts1.arff\"]\n",
    "label_counts = [6, 6,14,101,101,101,374,28,25]\n",
    "select_feature=[1,1,1,0.01,0.01,0.01,1,0.01,0.01]\n",
    "data_specific_sampling=[0.05,0.1,0.3,0.5,0.7]\n",
    "for i, path_to_arff_file in enumerate(path_to_arff_files):\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[i],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(select_feature[i])\n",
    "    y,label_names=LabelSelect()\n",
    "    dataset_name = path_to_arff_file.split(\".\")[0] \n",
    "    print(dataset_name)\n",
    "    for c_idx in range(1,5):\n",
    "        results_list=[]\n",
    "        print (f\"classifierid: {c_idx}\")\n",
    "        MacroF_list = []\n",
    "        MacroAUCROC_list = []\n",
    "        MacroAUCPR_list = []\n",
    "        RankLoss_list = []\n",
    "        res2_list = []\n",
    "        res4_list = []\n",
    "        res6_list = []\n",
    "        res3_list = []\n",
    "        for j in paracombine:\n",
    "            optmParameter = {\n",
    "                            'alpha':j[0], \n",
    "                            'beta': j[1],  \n",
    "                            'gamma': 0.01,  \n",
    "                            'lamda': 2**(4),  \n",
    "                            'lamda2': 2**(6), \n",
    "                            'maxIter': 20, \n",
    "                            'minimumLossMargin': 0.01, \n",
    "                            'bQuiet': 1\n",
    "                            }\n",
    "            dataname=dataset_name+str(j[0])+str(j[1])\n",
    "#             print (f\"'alpha': {alpha_id},'beta': {beta_id}\")\n",
    "            for sp in data_specific_sampling:\n",
    "                MacroF,MacroAUCROC,MacroAUCPR,RankLoss,Avgprecison,res2,res4,res6,res3,res7=training(c_idx,11,sp,dataname,optmParameter)                        \n",
    "                MacroF_list.append(MacroF)\n",
    "                MacroAUCROC_list.append(MacroAUCROC)\n",
    "                MacroAUCPR_list.append(MacroAUCPR)\n",
    "                RankLoss_list.append(RankLoss)\n",
    "                res2_list.append(res2)\n",
    "                res4_list.append(res4)\n",
    "                res6_list.append(res6)\n",
    "                res3_list.append(res3)\n",
    "                result_dict = { \n",
    "                    \"Classifier ID\": c_idx,\n",
    "                    'alpha':j[0], \n",
    "                    'beta': j[1], \n",
    "                    \"Sampling Rate\": sp,\n",
    "                    \"Macro F1\": MacroF,\n",
    "                    \"Macro AUC ROC\": MacroAUCROC,\n",
    "                    \"Macro AUC PR\": MacroAUCPR,\n",
    "                    \"Rank Loss\": RankLoss,\n",
    "                    \"std1\":res2,\n",
    "                    \"std2\":res4,\n",
    "                    \"std3\":res6,\n",
    "                    \"std4\":res3,\n",
    "                }\n",
    "                results_list.append(result_dict)\n",
    "        max_MacroF = max(MacroF_list)\n",
    "        max_MacroAUCROC = max(MacroAUCROC_list)\n",
    "        max_MacroAUCPR = max(MacroAUCPR_list)\n",
    "        min_RankLoss = min(RankLoss_list)\n",
    "        res2_max = res2_list[MacroF_list.index(max_MacroF)]\n",
    "        res4_max = res4_list[MacroAUCROC_list.index(max_MacroAUCROC)]\n",
    "        res6_max = res6_list[MacroAUCPR_list.index(max_MacroAUCPR)]\n",
    "        res3_min = res3_list[RankLoss_list.index(min_RankLoss)]\n",
    "        print(max_MacroF, max_MacroAUCROC, max_MacroAUCPR, min_RankLoss, res2_max, res4_max, res6_max, res3_min)\n",
    "#         print(f\"MacroF: {max(MacroF_list)}, MacroAUCROC: {max(MacroAUCROC_list)}, MacroAUCPR: {max(MacroAUCPR_list)}, RankLoss: {min(RankLoss_list)},res2: {res2_list[MacroF_list.index(max(MacroF_list))]}, res4: {res4_list[MacroAUCROC_list.index(max(MacroAUCROC_list))]}, res6: {res6_list[MacroAUCPR_list.index(max(MacroAUCPR_list))]}, res3: {res3_list[RankLoss_list.index(min(RankLoss_list))]}\")      \n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df.to_csv(f\"{dataset_name}_{c_idx}.csv\", index=False)\n",
    "print(\"----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2098a6e",
   "metadata": {},
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
